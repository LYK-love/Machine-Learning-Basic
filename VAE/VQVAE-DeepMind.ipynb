{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install\n",
    "```sh\n",
    "pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "pip install dm-haiku optax\n",
    "python3 -m pip install tensorflow[and-cuda]\n",
    "pip install tensorflow_datasets\n",
    "pip install matplotlib\n",
    "pip install jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version 0.4.25\n",
      "Haiku version 0.0.12\n",
      "TF version 2.16.1\n"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pickle\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "print(\"JAX version {}\".format(jax.__version__))\n",
    "print(\"Haiku version {}\".format(hk.__version__))\n",
    "print(\"TF version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Cifar10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10 = tfds.as_numpy(tfds.load(\"cifar10\", split=\"train+test\", batch_size=-1))\n",
    "# del cifar10[\"id\"], cifar10[\"label\"]\n",
    "# jax.tree_util.tree_map(lambda x: f'{x.dtype.name}{list(x.shape)}', cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cifar10.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download BouncingBall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_248587/361217499.py:4: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(os.path.join(folder, filename))\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = imageio.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load your dataset\n",
    "image_folder = './images'\n",
    "custom_images = load_images_from_folder(image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data into Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_dict = jax.tree_util.tree_map(lambda x: x[:40000], cifar10)\n",
    "# valid_data_dict = jax.tree_util.tree_map(lambda x: x[40000:50000], cifar10)\n",
    "# test_data_dict = jax.tree_util.tree_map(lambda x: x[50000:], cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BouncingBall\n",
    "total_images = custom_images.shape[0]\n",
    "\n",
    "# For example, let's say you want a 70-15-15 split\n",
    "train_end = int(total_images * 0.7)\n",
    "valid_end = train_end + int(total_images * 0.15)\n",
    "\n",
    "train_images = custom_images[:train_end]\n",
    "valid_images = custom_images[train_end:valid_end]\n",
    "test_images = custom_images[valid_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1400\n"
     ]
    }
   ],
   "source": [
    "print(total_images, train_images.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {'image': train_images}\n",
    "valid_data_dict = {'image': valid_images}\n",
    "test_data_dict = {'image': test_images}\n",
    "train_data_variance = np.var(train_data_dict['image'] / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CIFAR10\n",
    "# def cast_and_normalise_images(data_dict):\n",
    "#   \"\"\"Convert images to floating point with the range [-0.5, 0.5]\"\"\"\n",
    "#   data_dict['image'] = (tf.cast(data_dict['image'], tf.float32) / 255.0) - 0.5\n",
    "#   return data_dict\n",
    "\n",
    "# train_data_variance = np.var(train_data_dict['image'] / 255.0)\n",
    "# print('train data variance: %s' % train_data_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_and_normalise_images(features):\n",
    "    # Assuming your images are uint8 [0, 255]\n",
    "    features['image'] = tf.cast(features['image'], tf.float32) / 255.0\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder & Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualStack(hk.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(ResidualStack, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._layers = []\n",
    "    for i in range(num_residual_layers):\n",
    "      conv3 = hk.Conv2D(\n",
    "          output_channels=num_residual_hiddens,\n",
    "          kernel_shape=(3, 3),\n",
    "          stride=(1, 1),\n",
    "          name=\"res3x3_%d\" % i)\n",
    "      conv1 = hk.Conv2D(\n",
    "          output_channels=num_hiddens,\n",
    "          kernel_shape=(1, 1),\n",
    "          stride=(1, 1),\n",
    "          name=\"res1x1_%d\" % i)\n",
    "      self._layers.append((conv3, conv1))\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    h = inputs\n",
    "    for conv3, conv1 in self._layers:\n",
    "      conv3_out = conv3(jax.nn.relu(h))\n",
    "      conv1_out = conv1(jax.nn.relu(conv3_out))\n",
    "      h += conv1_out\n",
    "    return jax.nn.relu(h)  # Resnet V1 style\n",
    "\n",
    "\n",
    "class Encoder(hk.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(Encoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._enc_1 = hk.Conv2D(\n",
    "        output_channels=self._num_hiddens // 2,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"enc_1\")\n",
    "    self._enc_2 = hk.Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"enc_2\")\n",
    "    self._enc_3 = hk.Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(3, 3),\n",
    "        stride=(1, 1),\n",
    "        name=\"enc_3\")\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    h = jax.nn.relu(self._enc_1(x))\n",
    "    h = jax.nn.relu(self._enc_2(h))\n",
    "    h = jax.nn.relu(self._enc_3(h))\n",
    "    return self._residual_stack(h)\n",
    "\n",
    "\n",
    "class Decoder(hk.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(Decoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._dec_1 = hk.Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(3, 3),\n",
    "        stride=(1, 1),\n",
    "        name=\"dec_1\")\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "    self._dec_2 = hk.Conv2DTranspose(\n",
    "        output_channels=self._num_hiddens // 2,\n",
    "        # output_shape=None,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_2\")\n",
    "    self._dec_3 = hk.Conv2DTranspose(\n",
    "        output_channels=3,\n",
    "        # output_shape=None,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_3\")\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    h = self._dec_1(x)\n",
    "    h = self._residual_stack(h)\n",
    "    h = jax.nn.relu(self._dec_2(h))\n",
    "    x_recon = self._dec_3(h)\n",
    "    return x_recon\n",
    "    \n",
    "\n",
    "class VQVAEModel(hk.Module):\n",
    "  def __init__(self, encoder, decoder, vqvae, pre_vq_conv1, \n",
    "               data_variance, name=None):\n",
    "    super(VQVAEModel, self).__init__(name=name)\n",
    "    self._encoder = encoder\n",
    "    self._decoder = decoder\n",
    "    self._vqvae = vqvae\n",
    "    self._pre_vq_conv1 = pre_vq_conv1\n",
    "    self._data_variance = data_variance\n",
    "\n",
    "  def __call__(self, inputs, is_training):\n",
    "    \n",
    "    print(f\"x shape: {inputs.shape}\")\n",
    "    \n",
    "    z = self._pre_vq_conv1(self._encoder(inputs))\n",
    "    print(f\"z_e shape: {z.shape}\")\n",
    "    \n",
    "    vq_output = self._vqvae(z, is_training=is_training)\n",
    "    \n",
    "    print(f\"z_q shape: {vq_output['quantize'].shape}\")\n",
    "    \n",
    "    x_recon = self._decoder(vq_output['quantize'])\n",
    "    # recon_error = jnp.mean((x_recon - inputs) ** 2) / self._data_variance\n",
    "    recon_error = jnp.mean((x_recon - inputs) ** 2)\n",
    "    \n",
    "    loss = recon_error + vq_output['loss']\n",
    "    return {\n",
    "        'z': z,\n",
    "        'x_recon': x_recon,\n",
    "        'loss': loss,\n",
    "        'recon_error': recon_error,\n",
    "        'vq_output': vq_output,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters.\n",
    "batch_size = 32\n",
    "image_size = 64 # BouncingBall\n",
    "\n",
    "# 100k steps should take < 30 minutes on a modern (>= 2017) GPU.\n",
    "num_training_updates = 30000\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "# These hyper-parameters define the size of the model (number of parameters and layers).\n",
    "# The hyper-parameters in the paper were (For ImageNet):\n",
    "# batch_size = 128\n",
    "# image_size = 128\n",
    "# num_hiddens = 128\n",
    "# num_residual_hiddens = 32\n",
    "# num_residual_layers = 2\n",
    "\n",
    "# This value is not that important, usually 64 works.\n",
    "# This will not change the capacity in the information-bottleneck.\n",
    "embedding_dim = 64\n",
    "\n",
    "# The higher this value, the higher the capacity in the information bottleneck.\n",
    "num_embeddings = 512\n",
    "\n",
    "# commitment_cost should be set appropriately. It's often useful to try a couple\n",
    "# of values. It mostly depends on the scale of the reconstruction cost\n",
    "# (log p(x|z)). So if the reconstruction cost is 100x higher, the\n",
    "# commitment_cost should also be multiplied with the same amount.\n",
    "commitment_cost = 0.25\n",
    "\n",
    "# Use EMA updates for the codebook (instead of the Adam optimizer).\n",
    "# This typically converges faster, and makes the model less dependent on choice\n",
    "# of the optimizer. In the VQ-VAE paper EMA updates were not used (but was\n",
    "# developed afterwards). See Appendix of the paper for more details.\n",
    "vq_use_ema = True\n",
    "\n",
    "# This is only used for EMA updates.\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "# # Data Loading.\n",
    "train_dataset = tfds.as_numpy(\n",
    "    tf.data.Dataset.from_tensor_slices(train_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .shuffle(10000)\n",
    "    .repeat(-1)  # repeat indefinitely\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(-1))\n",
    "valid_dataset = tfds.as_numpy(\n",
    "    tf.data.Dataset.from_tensor_slices(valid_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .repeat(1)  # 1 epoch\n",
    "    .batch(batch_size)\n",
    "    .prefetch(-1))\n",
    "\n",
    "# Build modules.\n",
    "def forward(data, is_training):\n",
    "  encoder = Encoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "  decoder = Decoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "  pre_vq_conv1 = hk.Conv2D(\n",
    "      output_channels=embedding_dim,\n",
    "      kernel_shape=(1, 1),\n",
    "      stride=(1, 1),\n",
    "      name=\"to_vq\")\n",
    "\n",
    "  if vq_use_ema:\n",
    "    vq_vae = hk.nets.VectorQuantizerEMA(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_embeddings=num_embeddings,\n",
    "        commitment_cost=commitment_cost,\n",
    "        decay=decay)\n",
    "  else:\n",
    "    vq_vae = hk.nets.VectorQuantizer(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_embeddings=num_embeddings,\n",
    "        commitment_cost=commitment_cost)\n",
    "    \n",
    "  model = VQVAEModel(encoder, decoder, vq_vae, pre_vq_conv1,\n",
    "                     data_variance=train_data_variance)\n",
    "\n",
    "  return model(data['image'], is_training)\n",
    "\n",
    "forward = hk.transform_with_state(forward)\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, state, opt_state, data):\n",
    "  def adapt_forward(params, state, data):\n",
    "    # Pack model output and state together.\n",
    "    model_output, state = forward.apply(params, state, None, data, is_training=True)\n",
    "    loss = model_output['loss']\n",
    "    return loss, (model_output, state)\n",
    "\n",
    "  grads, (model_output, state) = (\n",
    "      jax.grad(adapt_forward, has_aux=True)(params, state, data))\n",
    "\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "\n",
    "  return params, state, opt_state, model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_e shape: (32, 16, 16, 64)\n",
      "z_q shape: (32, 16, 16, 64)\n",
      "z_e shape: (32, 16, 16, 64)\n",
      "z_q shape: (32, 16, 16, 64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:14\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/pjit.py:248\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 248\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_params_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    251\u001b[0m   maybe_fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m    252\u001b[0m       executable, out_tree, args_flat, out_flat, attrs_tracked)\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/pjit.py:143\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39minit_states, \u001b[38;5;241m*\u001b[39margs_flat]\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    145\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/core.py:2727\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2723\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2724\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2725\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2726\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/core.py:423\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 423\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/core.py:913\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 913\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/pjit.py:1409\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1406\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1407\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xla_extension_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m226\u001b[39m:\n\u001b[0;32m-> 1409\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_argnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_arg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxla_extension_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m229\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_shard_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1413\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_explicit_sharding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m xc\u001b[38;5;241m.\u001b[39m_xla\u001b[38;5;241m.\u001b[39mpjit(name, f, call_impl_cache_miss, [], [], donated_argnums,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m                       tree_util\u001b[38;5;241m.\u001b[39mdispatch_registry,\n\u001b[1;32m   1417\u001b[0m                       _get_cpp_global_cache(has_explicit_sharding))(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/pjit.py:1392\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1392\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m      \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m      \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1397\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1398\u001b[0m       compiled, tree_structure(out_flat), args, out_flat, [])\n\u001b[1;32m   1399\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out_flat, fastpath_data\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/pjit.py:1325\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _most_recent_pjit_call_executable\n\u001b[1;32m   1321\u001b[0m in_shardings \u001b[38;5;241m=\u001b[39m _resolve_in_shardings(\n\u001b[1;32m   1322\u001b[0m     args, in_shardings, out_shardings,\n\u001b[1;32m   1323\u001b[0m     resource_env\u001b[38;5;241m.\u001b[39mphysical_mesh \u001b[38;5;28;01mif\u001b[39;00m resource_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1325\u001b[0m compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m _most_recent_pjit_call_executable\u001b[38;5;241m.\u001b[39mweak_key_dict[jaxpr] \u001b[38;5;241m=\u001b[39m compiled\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:2271\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MeshExecutable:\n\u001b[1;32m   2270\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2271\u001b[0m     executable \u001b[38;5;241m=\u001b[39m \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2275\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:2734\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2731\u001b[0m       mesh \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mmesh  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2732\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m xla_executable, compile_options \u001b[38;5;241m=\u001b[39m \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_replicated\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2741\u001b[0m   semantics_in_shardings \u001b[38;5;241m=\u001b[39m SemanticallyEqualShardings(in_shardings)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:2591\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, _allow_propagation_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)\u001b[0m\n\u001b[1;32m   2586\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, compile_options\n\u001b[1;32m   2588\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2590\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2591\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable, compile_options\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/compiler.py:332\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compile_and_write_autotune_config(\n\u001b[1;32m    323\u001b[0m       backend,\n\u001b[1;32m    324\u001b[0m       computation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m       cache_key,\n\u001b[1;32m    330\u001b[0m   )\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/compiler.py:466\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[1;32m    458\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[1;32m    459\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    464\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[1;32m    465\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 466\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    470\u001b[0m   _cache_write(\n\u001b[1;32m    471\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[1;32m    472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/profiler.py:336\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/miniconda3/envs/VQVAE/lib/python3.9/site-packages/jax/_src/compiler.py:237\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    233\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_losses = []\n",
    "train_recon_errors = []\n",
    "train_perplexities = []\n",
    "train_vqvae_loss = []\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "train_dataset_iter = iter(train_dataset)\n",
    "params, state = forward.init(rng, next(train_dataset_iter), is_training=True)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for step in range(1, num_training_updates + 1):\n",
    "  data = next(train_dataset_iter)\n",
    "  params, state, opt_state, train_results = (\n",
    "      train_step(params, state, opt_state, data))\n",
    "\n",
    "  train_results = jax.device_get(train_results)\n",
    "  train_losses.append(train_results['loss'])\n",
    "  train_recon_errors.append(train_results['recon_error'])\n",
    "  train_perplexities.append(train_results['vq_output']['perplexity'])\n",
    "  train_vqvae_loss.append(train_results['vq_output']['loss'])\n",
    "\n",
    "  if step % 100 == 0:\n",
    "    print(f'[Step {step}/{num_training_updates}] ' + \n",
    "          ('train loss: %f ' % np.mean(train_losses[-100:])) +\n",
    "          ('recon_error: %.9f ' % np.mean(train_recon_errors[-100:])) +\n",
    "          ('perplexity: %.9f ' % np.mean(train_perplexities[-100:])) +\n",
    "          ('vqvae loss: %.9f' % np.mean(train_vqvae_loss[-100:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/VQ-VAE-DeepMind—BouncingBall.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming params and state are your model's parameters and state to save\n",
    "model_dict = {\n",
    "    'params': params,\n",
    "    'state': state,\n",
    "}\n",
    "\n",
    "# Choose a file name\n",
    "\n",
    "\n",
    "# Use pickle to save the model_dict to a file\n",
    "# with open(checkpoint_path, 'wb') as file:\n",
    "#     pickle.dump(model_dict, file)\n",
    "\n",
    "# print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pickle to load the model_dict from the file\n",
    "with open(checkpoint_path, 'rb') as file:\n",
    "    loaded_model_dict = pickle.load(file)\n",
    "\n",
    "# Extract the params and state\n",
    "loaded_params = loaded_model_dict['params']\n",
    "loaded_state = loaded_model_dict['state']\n",
    "\n",
    "params = loaded_params\n",
    "state = loaded_state\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BouncingBall, minimal loss can be 0.00001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16,8))\n",
    "ax = f.add_subplot(1,2,1)\n",
    "ax.plot(train_recon_errors)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('NMSE.')\n",
    "\n",
    "ax = f.add_subplot(1,2,2)\n",
    "ax.plot(train_perplexities)\n",
    "ax.set_title('Average codebook usage (perplexity).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "train_batch = next(iter(train_dataset))\n",
    "valid_batch = next(iter(valid_dataset))\n",
    "\n",
    "# Put data through the model with is_training=False, so that in the case of \n",
    "# using EMA the codebook is not updated.\n",
    "train_reconstructions = forward.apply(params, state, rng, train_batch, is_training=False)[0]['x_recon']\n",
    "valid_reconstructions = forward.apply(params, state, rng, valid_batch, is_training=False)[0]['x_recon']\n",
    "\n",
    "\n",
    "\n",
    "def convert_batch_to_image_grid(image_batch, rows=4, cols=8):\n",
    "    # Assuming image_batch is of shape (B, H, W, C)\n",
    "    B, H, W, C = image_batch.shape\n",
    "    assert B >= rows * cols, \"Not enough images to fill the grid\"\n",
    "    \n",
    "    reshaped = image_batch[:rows * cols].reshape(rows, cols, H, W, C)\n",
    "    reshaped = reshaped.transpose(0, 2, 1, 3, 4)  # Transpose to (rows, H, cols, W, C)\n",
    "    grid = reshaped.reshape(rows * H, cols * W, C)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "# Assuming 'train_batch', 'train_reconstructions', 'valid_batch', and 'valid_reconstructions' are available\n",
    "f = plt.figure(figsize=(16, 16))\n",
    "\n",
    "# Training Data Originals\n",
    "ax = f.add_subplot(2, 2, 1)\n",
    "ax.imshow(convert_batch_to_image_grid(train_batch['image']), interpolation='nearest')\n",
    "ax.set_title('Training Data Originals')\n",
    "plt.axis('off')\n",
    "\n",
    "# Training Data Reconstructions\n",
    "ax = f.add_subplot(2, 2, 2)\n",
    "ax.imshow(convert_batch_to_image_grid(train_reconstructions), interpolation='nearest')\n",
    "ax.set_title('Training Data Reconstructions')\n",
    "plt.axis('off')\n",
    "\n",
    "# Validation Data Originals\n",
    "ax = f.add_subplot(2, 2, 3)\n",
    "ax.imshow(convert_batch_to_image_grid(valid_batch['image']), interpolation='nearest')\n",
    "ax.set_title('Validation Data Originals')\n",
    "plt.axis('off')\n",
    "\n",
    "# Validation Data Reconstructions\n",
    "ax = f.add_subplot(2, 2, 4)\n",
    "ax.imshow(convert_batch_to_image_grid(valid_reconstructions), interpolation='nearest')\n",
    "ax.set_title('Validation Data Reconstructions')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
